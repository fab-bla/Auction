---
title: "First-Price Procurement Auctions"
output: 
  github_document:
    pandoc_args: --webtex
    toc: False
    toc_depth: 2
    number_sections: False
---

```{r, results = "hide", message = FALSE, warning = FALSE, include = FALSE}
# knitr options
knitr::opts_chunk$set(echo = TRUE,
                      fig.pos = "center",
                      fig.width = 9,
                      fig.height = 6,
                      fig.pos = "H",
                      dev.args = list(png = list(type = "cairo")),
                      dpi = 700)

# aux 
source("./Code/Misc/Auxilliary.R")
source("./Code/Misc/model_eval.R")

# packages
get.package(c("lubridate", "patchwork", "Metrics", "ggplot2"))
```

This repository contains my master thesis on first-price sealed-bid procurement 
auctions. In particular, the aim is to predict award prices of auctions held by the [Colorado Department of Transportation](https://codot.gov/business/bidding/bid-tab-archives). To date, the following models with varying preprocessing schedules have been compared:

* [Elastic Net](https://github.com/Base-R-Best-R/Auction/blob/main/Code/Models/Colab/CV_elastic_net.ipynb)
* Linear Model
  - [with logistic PCA preprocessing (parallel CV)](https://github.com/Base-R-Best-R/Auction/blob/main/Code/Models/Colab/CV_PreProcess_LM.ipynb)
* Random Forest
  - [with recursive feature elimination (parallel CV)](https://github.com/Base-R-Best-R/Auction/blob/main/Code/Models/Colab/CV_RecursiveFeatureElimination_RF.ipynb)
  - [with logistic PCA preprocessing](https://github.com/Base-R-Best-R/Auction/blob/main/Code/Models/Colab/Nested_CV_PCA_RF.ipynb)
  - [with logistic PCA preprocessing (parallel CV)](https://github.com/Base-R-Best-R/Auction/blob/main/Code/Models/Colab/Parallel_NestedCV_RF.ipynb)
* [XGBoost](https://github.com/Base-R-Best-R/Auction/blob/main/Code/Models/Colab/XGboost.ipynb)

# Best Model (`r Sys.Date() |> format("%D")`)

The boxplots below display the out of sample predicted values for the models that have been trained so far. Further, as the Engineers Estimate may be considered as a benchmark for prediction it is also included in the plot.

```{r, echo = FALSE}
# data
per_lasso <- readRDS("./Data/Misc Data/Figure_Data/Lasso_Performance_r2.RDS")
per_RF <- readRDS("./Data/Misc Data/Figure_Data/logPCA_RF_pred.RDS")
per_rfe_RF <- readRDS("./Data/Misc Data/Figure_Data/rfe_rf_pred.RDS")
per_xgb <- readRDS("./Data/Misc Data/Figure_Data/xgb_pred.RDS")
per_LM <- readRDS("./Data/Misc Data/Figure_Data/logPCA_LM_pred.RDS")

# change RF class name
per_RF[,2] <- "logPCA_RF"

# add model name to xgb
per_xgb <- cbind(per_xgb, "XGB") |> data.frame()
per_LM <- cbind(per_LM, "logPCA_LM") |> data.frame()
per_rfe_RF <- cbind(per_rfe_RF, "rfe_RF") |> data.frame()

# lst
lst <- list(per_lasso, per_RF, per_rfe_RF, per_xgb, per_LM)

# bind
do.call(rbind, lapply(lst, \(df){
 
  # names to na
  names(df) <- rep(NA, ncol(df))
  
  # ret
  return(df)
  
})) |> setNames(c("Award_Price", "Model")) -> dat_per_plot

# class
dat_per_plot$Award_Price <- as.numeric(dat_per_plot$Award_Price)

# relevel
dat_per_plot$Model <- factor(dat_per_plot$Model, 
                             levels = c("Act.", "Lasso Reg.", "logPCA_LM","logPCA_RF", "rfe_RF", "XGB","Eng. Est."))

# col
col <- c("cornflowerblue", "darkgoldenrod1", "firebrick", "forestgreen",
         "lightgreen","darkmagenta", "darkslategrey")
# align
par(mar = c(2, 2, 2, 2) + 0.1)

# boxplot
boxplot(Award_Price ~ Model, xlab = "", ylab = "Award Price",
        ylim = c(0, 20e3), pch = 19, 
        outcol = col, 
        col = col, 
        main = "Quality of Prediction",
        data = dat_per_plot)
```

The boxplot shows that the Lasso regression seems to be able to predict the outliers the best when compared to the remaining models and the Engineer's Estimate.

```{r, echo = FALSE, fig.height = 8, warning = FALSE}
# split data
split_df <- split(dat_per_plot, dat_per_plot[, "Model"])

# data 
AWPs <- lapply(split_df, \(df) df[, "Award_Price"])
Act <- AWPs[[1]] 
AWPs[[1]] <- NULL

# generate plots
Map(\(Pred, title, bool, fill){
  
  # plot
  Act_vs_Pred(Act, Pred, 
            title = title, bg_alt = bool, size = 2.5, shape = 21, alpha = 0.5,
            outcol = "black", fill = fill) 
  
  
}, AWPs, c("Lasso Regression", "LM with logPCA Preprocessing", 
           "RF with logPCA Preprocessing", "RF with Recursive Feature Elimination", 
           "eXtreme Gradient Boosting",
           "Engineer's Estimate"), c(T, F, F, F, F, F), 
            col[-1]) -> plots 

# print plots
print((plots[[1]] + plots[[6]]) /
      (plots[[3]] + plots[[4]]) /
      (plots[[5]] + plots[[2]]))
```

The plots above display the actual vs. predicted values. These plots may be used to assess, whether certain models just predict the outliers better, which is evident from the boxplots of predicted values. We observe that the Lasso model seems to outperform the other models across most of the observations in the validation set.

```{r, echo = FALSE}
# split
sp_DF <- split(dat_per_plot, dat_per_plot[, "Model"]) |> lapply("[[", 1)
Actual <- sp_DF[[1]]
sp_DF[[1]] <- NULL

# calc RMSE and MAE
sapply(sp_DF, \(x){

  sapply(list(Metrics::rmse, Metrics::mae), \(fun){
  
      fun(Actual, x)
    
  }) |> setNames(c("RMSE", "MAE"))

}) |> knitr::kable(col.names = c("Lasso", "logPCA_LM", "logPCA_RF", "rfe_RF", "XGB", "Eng. Est."))
```

The performance comparison utilizing linear and quadratic loss functions further emphasizes the dominance of the Lasso model.

# Required Software

## Installing Tabulizer for R > 4.1 

Unfortunately, the implementation of the Java library tabula which is a package called tabulizer cannot be installed via *install.packages()* and further the installation [guide](https://github.com/ropensci/tabulizer) is not up to date. However, with a small adaption the installation works almost as described in the installation guide for windows 10.

```{r, echo = FALSE}
sessionInfo()
```

Three steps have to be altered:

* When using Chocolately to install Java via the command prompt specify `choco install jdk8 -y` instead of `choco install jdk7 -y` 
* Within R change `Sys.setenv(JAVA_HOME = "C:/Program Files/Java/jdk1.8.0_92")` to `Sys.setenv(JAVA_HOME = "C:/Program Files/Java/jdk1.8.0_211")`
* Then install via `remotes::install_github(c("ropensci/tabulizerjars", "ropensci/tabulizer"), INSTALL_opts = "--no-multiarch")`, **after** installing *rJava* the usual way, i.e., via `install.packages()`